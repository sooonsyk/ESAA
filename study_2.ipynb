{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "study_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtxrTfGH0q1WRmKytiZ2ZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooonsyk/ESAA/blob/main/study_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**GBM**\n",
        "- 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식 - 에이다 부스트, 그래디언트 부스트\n",
        "- GBM은 에이다부스트와 유사하나 가중치 업데이트를 경사 하강법을 이용, 오류값은 실제값-예측값, 오류식을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트"
      ],
      "metadata": {
        "id": "LTcoB-4hnT34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**XGBoost extra Gradient Boost**\n",
        "###**XGBoost 개요**\n",
        "- GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결\n",
        "- 병렬 CPU 환경에서 병렬 학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있음\n",
        "- 장점 : 뛰어난 예측 성능, GBM 대비 빠른 수행 시간, 과적합 규제, 나무 가지치기, 자체 내장된 교차 검증, 결손값 자체 처리"
      ],
      "metadata": {
        "id": "xoJVQIm_9zwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- plot_importace() : 피처의 중요도를 막대그래프 형식으로 나타냄, f1 스코어를 기반으로, 피처명을 제대로 알 수 없으므로 f0,f1와 같이 피처 순서별로 f자 뒤에 순서를 붙여서 X축에 피처들로 나열"
      ],
      "metadata": {
        "id": "V8wgT1YXQA1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LightGBM**\n",
        "- XGBoost보다 학습에 걸리는 시간이 훨씬 적고 메모리 사용량도 상대적으로 적음\n",
        "- 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉬움\n",
        "- 리프 중신 트리 분할 방식 사용 - 트리의 균형을 맞추지 않고 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적은 규칙 트리 생성됨\n",
        "###**하이퍼 파라미터 튜닝 방안**\n",
        "- num_leaves 의 개수를 중심으로 min_child_samples(min_data_in_leaf), max_depth를 함께 조정하면서 모델의 복잡도를 줄이는 것"
      ],
      "metadata": {
        "id": "-A0goPh3U7Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**스태킹 앙상블**\n",
        "- 스태킹은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅, 부스팅과 공통점을 가짐, 가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다는 점\n",
        "- 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식\n",
        "- 두 종류의 모델 필요 - 개별적인 기반 모델, 이 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델\n"
      ],
      "metadata": {
        "id": "ELAJWUwOlChf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CV 세트 기반의 스태킹**\n",
        "- 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트 이용\n",
        "1. 각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터를 생성\n",
        "2. 1에서 개별 모델들이 생성한 학습용 데이터를 모두 스태킹 형태로 합쳐서 메타 모델이 학습할 최종 학습용 데이터 세트를 생성함, 마찬가지로 각 모델들이 생성한 테스트용 데이터를 모두 스태킹 형태로 합쳐서 메타 모델이 예측할 최종 테스트 데이터 세트를 생성, 메타 모델은 최종적으로 생성된 학습 데이터 세트와 원본 학습 데이터의 레이블 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 테스트 데이터 세트를 예측하고, 원본 테스트 데이터의 레이블 데이터를 기반으로 평가"
      ],
      "metadata": {
        "id": "1L3GZlrPpxCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**언더 샘플링과 오버 샘플링의 이해**\n",
        "이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면에 정상 레이블을 가지는 데이터 건수는 매우 많기 때문에 일반적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉬움\n",
        "문제점 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요 - 오버 샘플링과 언더 샘플링\n",
        "언더 샘플링 : 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식\n",
        "오버 샘플링 : 이상 데이터와 같이 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법, 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식"
      ],
      "metadata": {
        "id": "rnFvL_i8WwgX"
      }
    }
  ]
}