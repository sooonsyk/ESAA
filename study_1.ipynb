{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "study_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZF2qVq8WYN/Eh9aD2cfMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooonsyk/ESAA/blob/main/study_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**헷갈렸던 부분**"
      ],
      "metadata": {
        "id": "mcQrnWC0P3Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) 머신러닝의 개념##\n",
        ": 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법\n",
        "\n",
        ": 복잡한 조건/규칙들이 다양한 형태로 결합하고 시시각각 변하면서 도저히 소프트웨어 코드로 로직을 구성하여 이들을 관통하는 일정한 패턴을 찾기 어려운 경우에 유용\n",
        "\n",
        ": 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출\n",
        "\n",
        "####지도학습 supervised learning\n",
        "분류\n",
        "회귀\n",
        "추천 시스템\n",
        "시각/음성 감지/인지\n",
        "텍스트 분석, NLP\n",
        "####비지도학습 un-supervised learning\n",
        "클러스터링\n",
        "차원 축소\n",
        "강화학습"
      ],
      "metadata": {
        "id": "RSxEBLveOrBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2)판다스\n",
        "###DataFrame\n",
        "\n",
        "명칭 기반 인덱싱과 위치 기반 인덱싱의 구분\n",
        "- 명칭 기반 인덱싱 : 칼럼의 명칭을 기반으로 위치를 지정하는 방식\n",
        "- 위치 기반 인덱싱 : 0을 출발점으로 하는 가로축, 세로축 좌표 기반의 행과 열 위치를 기반으로 데이터 지정, 행,열 값으로 정수가 입력됨\n",
        "\n",
        "  => DataFrame의 인덱스값은 명칭 기반 인덱싱으로 간주\n",
        "\n",
        "####DataFrame iloc[ ]연산자\n",
        "iloc[ ]: 위치 기반 인덱싱만 허용하기 때문에 행과 열 값으로 int 혹은 int형의 슬라이싱, 팬시 리스트 값을 입력해줘야 함\n",
        "- 슬라이싱과 팬시 인덱싱은 제공하다 명확한 위치 기반 인덱싱이 사용되어야 하는 제약으로 인해 불린 인덱싱은 제공하지 않음\n",
        "\n",
        "####DataFrame loc[ ] 연산자\n",
        "loc[ ] : 명칭 기반으로 데이터를 추출, 행 위치에는 DataFrame index값 열 위치에는 칼럼 명을 입력해줌\n",
        "- index가 숫자 형일 수 있기 때문에 명칭 기반이라고 무조건 문자열을 입력한다는 선입견 가져선 안 됨\n",
        "loc[ ]에 슬라이싱 기호를 적용하면 시작값 ~ 종료값-1 이 아니라 시작값 ~ 종료값임 - 명칭 기반 인덱스 특성상 명칭이 숫자형이 아닐 수 있기 때문에 -1 못함"
      ],
      "metadata": {
        "id": "CEDPYqF9PEZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**분류**"
      ],
      "metadata": {
        "id": "lEnDF2tVRDMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**분류의 개요**\n",
        "- 지도 학습 : 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식\n",
        "- 분류 : 지도학습의 대표적인 유형, 학습 데이터로 주어진 데이터의 피처와 레이블값(결정값, 클래스값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것\n",
        "\n",
        "    : 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것\n",
        "- 앙상블 방법 : 서로 다른 또는 같은 머신러닝 알고리즘을 결합한 형태도 있으나 일반적으로 배깅과 부스팅 방법으로 나뉨\n",
        "\n",
        "##**1.결정 트리**\n",
        "- 결정 트리 : ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘, 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것, 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우\n",
        "  - 규칙 노드 : 규칙 조건\n",
        "  - 리프 노드 : 결정된 클래스 값\n",
        "  - 새로운 규칙 조건마다 서브트리가 생성됨\n",
        "  - 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어짐, 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기로 과적합으로 이어지기 쉬움, 즉 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음\n",
        "  - 가능한 한 적은 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 함, 이를 위해서는 어떻게 트리를 분할할 것인가가 중요, 최대한 균일한 데이터 새트를 구성할 수 있도록 분할하는 것이 필요\n",
        "  - 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건 만듦\n",
        "    - 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측\n",
        "    - 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보이득지수와 지니계수가 있음\n",
        "      - 정보이득지수 : 엔트로피는 주어진 데이터 집합의 혼잡도를 의미하는데, 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔티로피가 낮음, 정보이득지수는 1-엔트로피지수, 결정 트리는 정보 이득이 높은 속성을 기준으로 분할\n",
        "      - 지니계수 : 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수, 0이 가장 평등하고 1로 갈수록 불평등, 지니계수가 낮을수록 데이터의 균일도가 높은 것이므로 지니 계수가 낮은 속성을 기준으로 분할\n",
        "\n",
        "###**결정 트리 모델의 특징**\n",
        "- 결정 트리의 가장 큰 장점은 정보의 균일도라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적임, 룰이 매우 정확하고 이에 기반해 어덯게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화로 표현까지 할 수 있음, 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없음\n",
        "- 가장 큰 단점은 과적합으로 정확도가 떨어진다는 점, 피처 정보의 균일도에 따른 룰 규칙으로 서브 트리를 계속 만들다 보면 피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해질 수밖에 없음\n",
        "\n",
        "###**결정 트리 파라미터**\n",
        "- 사이킷런은 결정 트리 알고리즘은 DecisionTreeClassifier와 DecisionTreeRegressor 클래스 제공 - 분류를 위한 클래스와 회귀를 위한 클래스\n",
        "- 사이킷런의 결정 트리 구현은 CART(Classification And Regression Trees)알고리즘 기반 - 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘\n",
        "- DecisionTreeClassifier\n",
        "  - min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용, 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가, 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가\n",
        "  - min_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 데이터 수, 과적합 제오 용도 그러나 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요\n",
        "  - max_features : 최적의 분할을 위해 고려할 최대 피처 개수, 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행\n",
        "    - int : 대상 피처의 개수, float : 전체 피처 중 대상 피처의 퍼센트, sqrt : sqrt(전체 피처 개수), auto : sqrt와 동일, log : 전체 피처 중 log2(전체 피처 개수) 선정, None : 전체 피처 선정\n",
        "  - max_depth : 트리의 최대 높이 규정, 디폴트 None\n",
        "  - max_leaf_nodes : 말단 노드의 최대 개수\n",
        "\n",
        "###**결정 트리 모델의 시각화**\n",
        "- Graphviz 패키지 사용 - 그래프 기반의 dot 파일로 기술된 다양한 이미지를 쉽게 시각화할 수 있는 패키지, 사이킷런은 쉽게 인터페이스 할 수 있도록 export_graphviz() 제공, 함수 인자로 학습이 완료된 Estimator, 피처의 이름 리스트, 레이블 이름 리스트를 입력하면 학습된 결정 트리 규칙을 실제 트리 형태로 시각화해 보여줌"
      ],
      "metadata": {
        "id": "DKwm4OJeKM0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.앙상블 학습**\n",
        "###**앙상블 학습 개요**\n",
        "- 앙상블 학습을 통한 분류는 여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법\n",
        "- 이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 뛰어난 성능을 보이고 있지만, 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 보임\n",
        "- 앙상블 학습의 유형은 전통적으로 보팅, 배깅, 부스팅 세 가지로 나눌 수 있으면 이 외에도 스태깅을 포함한 다양한 방법이 있음\n",
        "  - 보팅과 배깅은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식\n",
        "    - 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합하는 것\n",
        "    - 배깅은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것 - 대표적으로 랜덤 포레스트\n",
        "      - 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출하는데 이렇게 개별 classifier에게 데이터를 샘플링해서 추출하는 방식을 부트스트래핑 분할 방식이라고 함, 중첩 허용\n",
        "    - 부스팅은 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 것\n",
        "\n",
        "###**보팅 유형 - 하드 보팅과 소프트 보팅**\n",
        "- 하드 보팅을 이용한 분류는 다수결 원칙과 비슷, 예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 것\n",
        "- 소프트 보팅은 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정, 일반적으로 보팅 방법으로 적용됨\n",
        "\n",
        "###**보팅 분류기**\n",
        "- 사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스 제공함\n"
      ],
      "metadata": {
        "id": "VKNawcVRX9M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. 랜덤 포레스트**\n",
        "- 앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있으며 다양한 영역에서 높은 예측 성능 보임\n",
        "- 기반 알고리즘은 결정 트리로서, 결정 트리의 쉽고 직관적인 장점을 그대로 가짐\n",
        "- 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 함\n",
        "- 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트 - 부트스트래핑, 서브세트의 데이터 건수는 전체 데이터 건수와 동일하지만 개별 데이터가 중첩되어 만들어짐\n",
        "- 사이킷런은 RandomForestClassifier 클래스를 통해 랜덤포레스트 기반의 분류 지원"
      ],
      "metadata": {
        "id": "P7JP1R5TdlXO"
      }
    }
  ]
}