{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "study_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwahqPhghmVLh3yqB+rxxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooonsyk/ESAA/blob/main/study_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**회귀**\n",
        "- 회귀 : 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법\n",
        "  - 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법\n",
        "  - 머신러닝 관점에서 독립변수는 피처에 해당하며 종속변수는 결정 값\n",
        "  - 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀계수를 찾아내는 것\n",
        "  - 회귀계수가 선형이냐 아니냐에 따라 선형회귀/비선형회귀\n",
        "  - 독립변수의 개수가 한 개면 단일회귀/ 여러개면 다중회귀\n",
        "- 지도학습은 분류와 회귀 두 가지 유형으로 나뉨 - 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자 값임\n",
        "- 선형회귀 : 실제값과 에측값의 차이(오류의 제곱값)를 최소화하는 직선형 회귀선을 최적화하는 방식, 가장 많이 사용됨\n",
        "  - 규제 방법에 따라 다시 별도의 유형으로 나뉨\n",
        "    - 규제 : 일반적은 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 페널티 값을 적용하는 것\n",
        "  - 일반 선형 회귀 : 예측값과 실제값의 RSS를 최소화할 수 있도록 회귀 계수를 최적화하며 규제를 적용하지 않은 모델\n",
        "  - 릿지 : 선형 회귀에 L2 규제 추가, 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀계수값을 더 작게 만드는 규제 모델\n",
        "  - 라쏘 : 선형 회귀에 L1 규제 추가, 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측시 피처가 선택되지 않게 하는 것\n",
        "  - 엘라스틱넷 : L2, L1 규제를 함께 결합한 모델, 주로 피처가 많은 데이터 세트에서 적용, L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기 조정\n",
        "      - $RSS(W) + alpha2*||W||^2_2 + alpha1 * ||W||_1$ 식을 최소화하는 W 찾는 것이 엘라스틱넷 회귀 비용함수의 목표\n",
        "      - 오래 걸림\n",
        "      - l1_ratio 는 a/(a+b) - 0이면 L2 규제, 1이면 L1 규제\n",
        "  - 로지스틱 회귀 : 사실은 분류에 사용되는 선형 모델\n",
        "      - 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환값을 확률로 간주해 확률에 따라 분류 결정\n",
        "       - x값이 아무리 커지거나 작아져도 y값은 항상 0과 1 사이 값 반환"
      ],
      "metadata": {
        "id": "DIBvjIB9KgxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**차원 축소**\n",
        "##**차원 축소 개요**\n",
        "- 차원 축소 : 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것\n",
        "  - 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, sparse한 구조를 가지게 됨 \n",
        "  - 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도 떨어짐, 피처가 많을 경우 개별 피처 간에 상관관계가 높을 가능성 큼\n",
        "- 피처 선택과 피처 추출로 나눔\n",
        "  - 피처 선택 : 특성 선택, 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고 데이터의 특징을 잘 나타내는 주요 피처만 선택\n",
        "  - 피처 추출 : 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것, 기존 피처를 단순 압축이 아닌 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것\n",
        "    - 기존 피처가 전혀 인지하기 어려웠던 잠재적인 요소를 추출하는 것\n",
        "- 차원 축소는 단순히 데이터의 압축을 의미하는 것이 아님, 더 중요한 의미는 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 있음\n",
        "- 매우 많은 픽셀로 이루어진 이미지 데이터에서 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축 수행 가능\n",
        "- 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱 의미나 토픽을 잠재 요소로 간주하고 이를 찾아낼 수 있음"
      ],
      "metadata": {
        "id": "Iiul_-ig8g2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**PCA(Principal Component Analysis)**\n",
        "###**PCA 개요**\n",
        "- PCA : 가장 대표적인 차원 축소 기법, 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법\n",
        "  - 기존 데이터의 정보 유실이 최소화 되는 것이 당연\n",
        "  - 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하는데 이것이 PCA의 주성분이 됨 - 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주\n",
        "  -  가장 큰 데이터 변동성을 기반으로 첫번째 축을 생성하고, 두 번째 축은 이 벡터 축에 직각이 되는 벡터를 축으로 함, 세 번째 축은 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축 생성\n",
        "  - 만들어진 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소됨\n",
        "  - 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석\n",
        "  - 선형대수 관점에서 해석하면, 입력 데이터의 공분산 행렬을 고유값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것 - 고유 벡터가 주성분 벡터로서 입력 데이터의 분산이 큰 방향을 나타냄, 고윳값은 고유 벡터의 크기와 입력 데이터의 분산 나타냄\n",
        "  - 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변환하는 방식\n",
        "  1. 입력데이터 세트의 공분산 행렬 생성\n",
        "  2. 공분산 행렬의 고유벡터와 고유값을 계산\n",
        "  3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유 벡터 추출\n",
        "  4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력된 데이터 변환"
      ],
      "metadata": {
        "id": "dsmVIyC--l9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LDA(Linear Discriminant Analysis)**\n",
        "###**LDA 개요**\n",
        "- LDA : 선형 판별 분석법, 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소, 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾음\n",
        "- 클래스 간 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 차원을 축소함 - 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산의 최대한 작게 가져가는 방식\n",
        "1. 클래스 내부와 클래스 간 분산 행렬 구함, 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 벡터를 기반으로 구함\n",
        "2. 클래스 내부 분산 행렬을 $S_W$, 클래스 간 분산 행렬을 $S_B$라고 하면 다음 식으로 두 행렬을 고유벡터로 분해\n",
        "3. 고유값이 가장 큰 순으로 K개 만큼 추출\n",
        "4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환함"
      ],
      "metadata": {
        "id": "C21ApkTLRlla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**SVD(Singular Value Decomposition)**\n",
        "###**SVD 개요**\n",
        "- 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 잇음, 특이값 분해\n",
        "- $A=UΣV^T$\n",
        "- mxn 행렬 A를 위와 같이 분해하는 것 의미\n",
        "- 행렬 U, V에 속한 벡터는 특이벡터, 모든 특이 벡터는 서로 직교하는 성질 가짐, $\\Sigma$는 대각행렬이며 행렬의 대각에 위치한 값만 0이 아니고 나머지 위치의 값은 모두 0, 0이 아닌 값이 행렬A의 특이 값\n",
        "- Truncated SVD 는 $\\Sigma$의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U,V의 원소도 함께 제거헤 더욱 차원을 줄인 형태로 분해하는 것"
      ],
      "metadata": {
        "id": "5UuYCMcbUo7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**NMF(Non-Negative Matrix Factorization)**\n",
        "###**NMF 개요**\n",
        "- 낮은 랭크를 통한 행렬 근사 방식의 변형\n",
        "- 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법\n",
        "- 분해될 행렬은 잠재 요소를 특성으로 가지게 됨"
      ],
      "metadata": {
        "id": "6DzpkiQHrObK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**군집화**\n",
        "##**K-평균 알고리즘 이해**\n",
        "- K-평균 : 군집 중심 centroid이라는 특정한 임의의 지점을 선책해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기법\n",
        "- 군집 중심점 : 선택된 포인트의 평균 지점으로 이동하고 이동된 중심점에서 다시 가까운 포인트를 선택, 다시 중심점을 평균 지점으로 이동하는 프로세스 반복적으로 수행\n",
        "  - 모든 데이터 포인트에서 더이상 중심점의 이동이 없을 경우에 반복을 멈추고 해당 중심점에 속하는 데이터 포인트들을 군집화하는 기법\n",
        "- 장점 : 가장 많이 활용되고 쉽고 간결함\n",
        "- 단점 : 거리 기반 알고리즘으로 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어짐 - 이를 위해 PCA로 차원 감소를 적용해야 할 수도 있음, 반복 횟수가 많을 경우 수행 시간이 매우 느려짐, 몇개의 군집을 선택해야할지 가이드하기 어려움\n",
        "\n",
        "###**사이킷런 KMeans 클래스 소개**\n",
        "- 사이킷런 패키지는 K-평균을 구현하기 위해 KMeans 클래스 제공\n",
        "- class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto') 이와 같은 초기화 파라미터 가짐\n",
        "  - 가장 중요한 파라미터 n_clusters로 군집화할 개수, 군집 중심점의 개수 의미\n",
        "  - init 은 초기에 군집 중심점의 좌쵸를 설정할 방식 의미, 보통 임의로 중심을 설정하지 않고 k-means++ 방식으로 최초 설정\n",
        "  - max_iter는 최대 반복 횟수이며, 이 횟수 이전에 모든 데이터의 중심점 이동이 없으면 종료\n",
        "- 사이킷런의 비지도학습 클래스와 마찬가지로 fit(데이터 세트) 또는 fit_transform(데이터 세트) 메서드 이용해 수행\n",
        "- 수행된 KMeans 객체는 군집화 수행이 완료돼 관련된 주요 속성 알 수 있음\n",
        "  - labels_ : 각 데이터 포인트가 속한 군집 중심점 레이블\n",
        "  - cluster_centers_ : 각 군집 중심점 좌표(Shape는 [군집 개수, 피처 개수])\n"
      ],
      "metadata": {
        "id": "se7znPqJbpl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**평균 이동**\n",
        "###**평균 이동 Mean Shift의 개요**\n",
        "- 평균 이동 : K-평균과 유사하게 중심을 군집의 중심으로 지속적으로 움직이면서 군집화 수행\n",
        "  - K-평균이 중심에 소속된 데이터의 평균 거리 중심으로 이동하는 데 반해, 평균 이동은 중심을 데이터가 모여 있는 밀도가 가장 높은 곳으로 이동시킴\n",
        "  - 데이터의 분포도를 이용해 군집 중심점 찾음, 확률 밀도 함수 이용, KDE 이용\n",
        "  - KDE Kernal Density Estimation 는 커널 함수를 통해 어떤 변수의 확률 밀도 함수를 측정, 관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수 측정\n",
        "    - 확률밀도함수 PDF : 확률 변수의 분포를 나타내는 함수\n",
        "    - 개별 관측 데이터에 커널 함수 적용한 뒤, 이 적용 값을 모두 더한 후 개별 관측 데이터의 건수로 나눠 확률 밀도 함수를 추정하며 대표적인 커널 함수로서 가우시안 분포 함수 사용\n",
        "   $KDE = {1\\over n} * \\sum_{i=1}^nK_h(x-x_i) = {1 \\over nh}\\sum_{i=1}^nK({x-x_i \\over h})$\n",
        "   - K는 커널함수 x는 확률 변숫값, xi는 관측값, h는 대역폭-KDE 형태를 부드러운/뾰족한 형태로 평활화 하는데 적용\n",
        "   - 대역폭이 클수록 적은 수의 군집 중심점을 가지며 대역폭이 적을수록 많은 수의 군집 중심점 가짐\n",
        "- 최적의 bandwidth 찾기 위해 estimate_bandwidth() 제공\n"
      ],
      "metadata": {
        "id": "8uAv0WpZ-mUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GMM Gaussian Mixture Model**\n",
        "###**GMM 소개**\n",
        "- 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정하에 군집화를 수행하는 방식\n",
        " - 가우시안 분포 : 정규분포, 좌우 대칭형의 종 형태를 가진 통계학에서 가장 잘 알려진 연속 확룰 함수\n",
        " - 데이터를 여러 개의 가우시안 분포가 섞인 것으로 간주, 섞인 데이터 분포에서 개별 유형의 가우시안 분포 추출\n",
        "- 모수 추정 : 여러 개의 정규 분포 곡선을 추출하고, 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정하는 방식, 대표적으로 2가지 추정\n",
        "  - 개별 정규 분포의 평균과 분산\n",
        "  - 각 데이터가 어떤 정규 분포에 해당되는지의 확률\n",
        "  - EM Expectation and Maximization 방법 적용"
      ],
      "metadata": {
        "id": "FVH8qmKiItOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DBSCAN**\n",
        "###**DBSCAN 개요**\n",
        "- 밀도 기반 군집화, 데이터의 분포가 기하학적으로 복잡한 데이터 세트에도 효과적인 군집화 가능\n",
        "  - 입실론 주변 영역 : 개별 데이터를 중심으로 입실론 반경을 가지는 원형의 영역\n",
        "  - 최소 데이터 개수 : 개별 데이터의 입실론 주변 영역에 포함되는 타 데이터의 개수\n",
        "- 입실론 주변 영역 내에 포함되는 최소 데이터 개수를 충족시키는가 아닌가에 따라 데이터 포인트 정의\n",
        "  - 핵심 포인트 : 주변 영역 내에 최소 데이터 개수 이상의 타 데이터를 가지고 있을 경우\n",
        "  - 이웃 포인트 : 주변 영역 내에 위치한 타 데이터\n",
        "  - 경계 포인트 : 주변 영역 내에 최소 데이터 개수 이상의 타 데이터를 가지고 있지 않지만 핵심 포인트를 이웃 포인트로 가지고 있는 데이터\n",
        "  - 잡음 포인트 : 최소 데이터 개수 이상의 이웃 포인트를 가지고 있지 않으며, 핵심 포인트도 이웃 포인트로 가지고 있지 않는 데이터\n",
        "- 입실론 주변 영역의 최소 데이터 개수를 포함하는 밀도 기준을 충족시키는 데이터인 핵심 포인트를 연결하면서 군집화를 구성하는 방식\n"
      ],
      "metadata": {
        "id": "W1r3iAUMSyyV"
      }
    }
  ]
}